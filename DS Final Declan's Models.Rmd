---
title: "DS Final Declan's Models"
author: "Declan Young"
date: "12/2/2021"
output: html_document
---

```{r setup, include=FALSE, cache = TRUE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
```

## Tuning Models {.tabset}

### KNN {.tabset}

```{r cars}
library(data.table)
library(class)
```

#### 25 Neighbors

```{r}
KNN_train = train[-c(3,5,12)]

train1h = one_hot(as.data.table(KNN_train),cols = "auto",sparsifyNAs = TRUE,naCols = TRUE,dropCols = TRUE,dropUnusedLevels = TRUE) 

KNN_tune = tune[-c(3,5,12)]

tune1h = one_hot(as.data.table(KNN_tune),cols = "auto",sparsifyNAs = TRUE,naCols = TRUE,dropCols = TRUE,dropUnusedLevels = TRUE) 

Music_25NN = knn(train = train1h,
                test = tune1h,
                cl = train$danceability,
                k = 25,
                use.all = TRUE,
                prob = TRUE)


confusionMatrix(as.factor(Music_25NN), as.factor(tune$danceability), positive = "top25", dnn=c("Prediction", "Actual"), mode = "sens_spec")


#Selecting the variables I predict would most closely correlate to danceability (popularity, energy, liveliness, loudness, and tempo)

Music_25NN_tuned = knn(train = train[c(1,4,6,7,10)],
                test = tune[c(1,4,6,7,10)],
                cl = train$danceability,
                k = 25,
                use.all = TRUE,
                prob = TRUE)

confusionMatrix(as.factor(Music_25NN_tuned), as.factor(tune$danceability), positive = "top25", dnn=c("Prediction", "Actual"), mode = "sens_spec")

#Worsened accuracy (especially sensitivity)
```

#### 100 Neighbors

```{r}

#Sensitivity drops a lot for a small improvement in specificity (likely more-so due to the imbalanced nature of the data than it predicting the observations better)
Music_100NN = knn(train = train1h,
                test = tune1h,
                cl = train$danceability,
                k = 100,
                use.all = TRUE,
                prob = TRUE)

confusionMatrix(as.factor(Music_100NN), as.factor(tune$danceability), positive = "top25", dnn=c("Prediction", "Actual"), mode = "sens_spec")

Music_100NN_tuned = knn(train = train[c(1,4,6,7,10)],
                test = tune[c(1,4,6,7,10)],
                cl = train$danceability,
                k = 25,
                use.all = TRUE,
                prob = TRUE)

confusionMatrix(as.factor(Music_100NN_tuned), as.factor(tune$danceability), positive = "top25", dnn=c("Prediction", "Actual"), mode = "sens_spec")
```

#### 5 Neighbors

```{r}
Music_5NN = knn(train = train1h,
                test = tune1h,
                cl = train$danceability,
                k = 5,
                use.all = TRUE,
                prob = TRUE)

confusionMatrix(as.factor(Music_5NN), as.factor(tune$danceability), positive = "top25", dnn=c("Prediction", "Actual"), mode = "sens_spec")

#Slightly increased sensitivity, but not enough and at the expense of accuracy

Music_5NN_tuned = knn(train = train[c(1,4,6,7,10)],
                test = tune[c(1,4,6,7,10)],
                cl = train$danceability,
                k = 5,
                use.all = TRUE,
                prob = TRUE)

confusionMatrix(as.factor(Music_5NN_tuned), as.factor(tune$danceability), positive = "top25", dnn=c("Prediction", "Actual"), mode = "sens_spec")


#Best model is likely the first model made (the increased in specificity were not worth the decreases in sensitivity) 

```

### Decision Tree

```{r pressure, echo=FALSE}
features = data.frame(train[-3])
target = train$danceability

  
#Creating a cross validation process that maximizes the F-1 Score
cv_train = trainControl(method = "repeatedcv",
                        number = 10,
                        repeats = 5,
                        returnResamp = "all",
                        classProbs = TRUE,
                        allowParallel = TRUE,)

grid_control = expand.grid(.winnow = c(TRUE,FALSE), 
                    .trials=c(1,5,10,15,20), 
                    .model="tree")

#Use this to get # of boosting iterations/winnowing through cross validation that prioritizes F-1 score (This is very computationally expensive)
dance_mdl = train(x=features,
                y=target,
                method="C5.0",
                tuneGrid=grid_control,
                trControl=cv_train)

save(plot(dance_mdl),file = "boosts.png")
save.image(file = )

#Results: 20 Boosting iterations with no winnowing

#Remaking the model for visualization purposes
c5_model = C5.0(danceability~.,
                    method = "class",
                    parms = list(split = "gini"),
                    data = train,
                    trials = 20,
                    control = C5.0Control(winnow = FALSE,
                                          minCases = 500))

varImp(c5_model)
```

```{r, fig.height=5, fig.width=10}
plot(c5_model)

print(" The first split is for genre; intuitively, this makes sense because of the booleans that follow. Rap and Hip hop will be most danceable when they are high energy/fast tempo while other genres may be more suited towards slow dancing that work best with low energy/slow tempo")

dance_prob = as_tibble(predict(c5_model, tune, type = "prob"))
dance_pred = predict(c5_model, tune, type = "class")

confusionMatrix(as.factor(dance_pred),
                as.factor(tune$danceability),
                dnn = c("Predicted", "Actual"),
                mode = "sens_spec",
                positive = "top25")

#table = table(as.factor(dance_pred),
#      as.factor(tune$danceability))
#(spec = table[1]/(table[1]+table[2]))

#F1 Score at .5 threshold
pred_5 = as_factor(ifelse(dance_prob$top25 > 0.5, "top25", "bottom75"))
print(paste("F-1 Score at a .5 threshold:",F1_Score(y_pred = pred_5, y_true = as_factor(tune$danceability), positive = "top25")))
```
#### Tuning the number of cases

```{r}

empty = data.frame(min_cases = character(),
                   F1 = numeric())

min_cases = function(x){
  c5_adj_model = C5.0(danceability~.,
                    method = "class",
                    parms = list(split = "gini"),
                    data = train,
                    trials = 20,
                    control = C5.0Control(winnow = FALSE,
                                          minCases = x,
                                          ))

  c5_adj_pred = as_tibble(predict(c5_adj_model, tune, type = "prob"))
  c5_adj_class = as_factor(predict(c5_adj_model, tune))
  pred_4_adj = as.factor(ifelse(c5_adj_pred$top25 > 0.5, "top25", "bottom75"))
  pred_4_adj = relevel(pred_4_adj, "bottom75")
  table = table(as.factor(c5_adj_class),
      as.factor(tune$danceability))
  spec = table[1]/(table[1]+table[2])
  F_1 = data.frame(min_cases = x,F1 = F1_Score(y_pred = pred_4_adj, y_true = as_factor(tune$danceability), positive = "top25"), specificity = spec)
  
  empty <<- rbind(empty, F_1)
}
cases = c(2,5,10,50,100,250,500,1000,2500)
lapply(cases, min_cases)

ggplot(empty, aes(x = reorder(as.factor(min_cases), -F1), y = F1))+ geom_col(width = .8)+ geom_bar(data=subset(empty, F1==max(F1)), aes(as.factor(min_cases), F1),
              fill="green", stat="identity")

ggplot(empty, aes(x = reorder(as.factor(min_cases), -specificity), y = specificity))+ geom_col(width = .8)+ geom_bar(data=subset(empty, specificity==max(specificity)), aes(as.factor(min_cases), specificity),
              fill="green", stat="identity")

#50 seems to be the best compromise
```

```{r}
empty2 = data.frame(CF_level = character(),
                   F1 = numeric())

CF_level = function(x){
  c5_adj_model = C5.0(danceability~.,
                    method = "class",
                    parms = list(split = "gini"),
                    data = train,
                    trials = 20,
                    control = C5.0Control(winnow = FALSE,
                                          minCases = 50,
                                          CF = x
                                          ))

  c5_adj_pred = as_tibble(predict(c5_adj_model, tune, type = "prob"))
  c5_adj_class = as_factor(predict(c5_adj_model, tune))
  pred_4_adj = as.factor(ifelse(c5_adj_pred$top25 > 0.5, "top25", "bottom75"))
  pred_4_adj = relevel(pred_4_adj, "bottom75")
  table = table(as.factor(c5_adj_class),
      as.factor(tune$danceability))
  spec = table[1]/(table[1]+table[2])
  F_1 = data.frame(CF_level = x,F1 = F1_Score(y_pred = pred_4_adj, y_true = as_factor(tune$danceability), positive = "top25"), specificity = spec)
  
  empty2 <<- rbind(empty2, F_1)
}
cases = c(.1,.2,.3,.4,.5,.6,.7,.8,.9)
lapply(cases, CF_level)

ggplot(empty2, aes(x = reorder(as.factor(CF_level), -F1), y = F1))+ geom_col(width = .8)+ geom_bar(data=subset(empty2, F1==max(F1)), aes(as.factor(CF_level), F1),
              fill="green", stat="identity")

ggplot(empty2, aes(x = reorder(as.factor(CF_level), -specificity), y = specificity))+ geom_col(width = .8)+ geom_bar(data=subset(empty2, specificity==max(specificity)), aes(as.factor(CF_level), specificity),
              fill="green", stat="identity")


c5_model_tune = C5.0(danceability~.,
                    method = "class",
                    parms = list(split = "gini"),
                    data = train,
                    trials = 20,
                    control = C5.0Control(winnow = FALSE,
                                          minCases = 50,
                                          CF = .6))

dance_prob_tune = as_tibble(predict(c5_model_tune, tune, type = "prob"))
dance_pred_tune = predict(c5_model_tune, tune, type = "class")

confusionMatrix(as.factor(dance_pred_tune),
                as.factor(tune$danceability),
                dnn = c("Predicted", "Actual"),
                mode = "sens_spec",
                positive = "top25")

#F1 Score at .5 threshold
pred_5_tune = as_factor(ifelse(dance_prob_tune$top25 > 0.5, "top25", "bottom75"))
print(paste("F-1 Score at a .5 threshold:",F1_Score(y_pred = pred_5_tune, y_true = as_factor(tune$danceability), positive = "top25")))

#Best model so far, should do roc curve to find threshold


dance_prob_tune = as_tibble(dance_prob_tune)
dance_eval_tune = tibble(pred_class=dance_pred_tune, pred_prob=dance_prob_tune$top25,target=as.numeric(tune$danceability))

pred = prediction(dance_eval_tune$pred_prob, dance_eval_tune$target)

#Choosing to evaluate the true positive rate and false positive rate based on the threshold
ROC_curve = performance(pred,"tpr","fpr")

plot(ROC_curve, colorize=TRUE)
abline(a=0, b= 1)

tree_perf_AUC = performance(pred,"auc")

print(paste("AUC =",tree_perf_AUC@y.values))

#.5 is near the elbow of the graph (maybe could try .4 and .6)
print("Thresholding at .4")

dance_pred_4 = as_factor(ifelse(dance_prob$top25 > 0.4, "top25", "bottom75"))

confusionMatrix(as.factor(dance_pred_4),
                as.factor(tune$danceability),
                dnn = c("Predicted", "Actual"),
                mode = "sens_spec",
                positive = "top25")

print("Increases in sensitivity are not worth the decreases in specificity because it is our metric of interest.")

print("Thresholding at .6")

dance_pred_6 = as_factor(ifelse(dance_prob$top25 > 0.6, "top25", "bottom75"))

confusionMatrix(as.factor(dance_pred_6),
                as.factor(tune$danceability),
                dnn = c("Predicted", "Actual"),
                mode = "sens_spec",
                positive = "top25")

print("The improvements in specificity are not worth the great losses in sensitivity.")
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

## Test Models

### KNN

```{r}

KNN_test = test[-c(3,5,12)]

test1h = one_hot(as.data.table(KNN_test),cols = "auto",sparsifyNAs = TRUE,naCols = TRUE,dropCols = TRUE,dropUnusedLevels = TRUE) 

Music_25NN_Final = knn(train = train1h,
                test = test1h,
                cl = train$danceability,
                k = 25,
                use.all = TRUE,
                prob = TRUE)


confusionMatrix(as.factor(Music_25NN_Final), as.factor(test$danceability), positive = "top25", dnn=c("Prediction", "Actual"), mode = "sens_spec")

#Establishing dataframe with both the prediction and probability
Music_25NN_Final_Prob = data.frame(pred = as_factor(Music_25NN_Final), prob = attr(Music_25NN_Final, "prob"))

#Adjusting so probability aligns with probability of "top25" for all observations
Music_25NN_Final_Prob$prob = ifelse(Music_25NN_Final_Prob$pred == "bottom75", 1 - Music_25NN_Final_Prob$prob, Music_25NN_Final_Prob$prob)

#Finding F1 score at .5 threshold
pred_5_KNN = as_factor(ifelse(Music_25NN_Final_Prob$prob > 0.5, "top25", "bottom75"))
print(paste("F-1 Score at a .5 threshold:",F1_Score(y_pred = pred_5_KNN, y_true = as_factor(test$danceability), positive = "top25")))
```

### Decision Tree

```{r}

dance_prob_test = as_tibble(predict(c5_model_tune, test, type = "prob"))
dance_pred_test = predict(c5_model_tune, test, type = "class")

confusionMatrix(as.factor(dance_pred_test),
                as.factor(test$danceability),
                dnn = c("Predicted", "Actual"),
                mode = "sens_spec",
                positive = "top25")

#F1 Score at .5 threshold
pred_5_test = as_factor(ifelse(dance_prob_test$top25 > 0.5, "top25", "bottom75"))
print(paste("F-1 Score at a .5 threshold:",F1_Score(y_pred = pred_5_test, y_true = as_factor(test$danceability), positive = "top25")))

```



---
title: "Predicting Music Danceability"
author: "Adam Kippenhan and Declan Young"
date: "12/8/2021"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    theme: journal
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)
```

# Introduction

The Charlottesville bars are looking for ways to increase their attendance next semester and are looking for different ways to do it. One area they have selected is the playlist of songs for their bar. If they are able to find songs that people will be more likely to dance to, this might increase their attendance and popularity. In this project, we will be looking into a data set of songs and creating machine learning models to try and predict a danceability rating for them. 

We will be using three different model types: k-nearest neighbors, (KNN), decision tree and random forest. For each model, we created an initial model and then tried to tune it to be able to best predict our data. We will be looking to predict whether a particular song will be in the top 25% percentile in terms of its danceability rating and will therefore be a song with optimal qualities to place on a playlist for nights at bars.

Two metrics that we will be focusing on in this project are specificity as well as the F1 score. Specificity is important for this situation because songs that are classified as being in the top 25% percentile when they should not be could be detrimental to the atmosphere of a bar and could turn away bar-goers. F1 score will also be important in evaluating our models as the data is imbalanced.

# Data Analysis

We obtained the dataset we are using from [Kaggle](https://www.kaggle.com/vicsuperman/prediction-of-music-genre). It contains songs from the major genres with various metrics about them including acousticness, energy, key, loudness, tempo, genre and several other metrics. Our models will use these various metrics to try and predict whether a song will be in the top 25% percentile in danceability rating. 

To clean our dataset to prepare to feed it into the models, we first removed variables that we decided not to use either because they were identifying values such as song name and artist or because they had erroneous data in them. For example, about half of the values in the column for the song's duration reported a duration of -1 milliseconds. We then removed NA values and converted the tempo column to numeric values. We also normalized the popularity, loudness and tempo columns to be between 0 and 1 using a simple min-max scaler. Next, we collapsed the factors of the key column to combine sharp and natural notes of the same type as well as classified the genre and mode columns as factors. We calculated the 75th percentile of the danceability scores and created a binary variable indicating whether a song was in the top 25% or not, replacing the original column with danceability scores.

Finally, we split our dataset into train, tune and test partitions for use with our models. Below are some summary statistics about important variables in our data as well as a table showing our final cleaned data.

```{r dataCleaning}
# load required libraries
library(C50)
library(caret)
library(class)
library(DT)
library(data.table)
library(MLmetrics)
library(mlbench)
library(mltools)
library(ROCR)
library(randomForest)
library(tidyverse)

music_genre_data <- read_csv("music_genre.csv")

# Removing identifier variables and mismanaged data columns (~50% of the duration variable indicated a song length of -1 ms)
music_genre = music_genre_data[-c(1,2,3,7,9,16)]

# convert '?' to NA
music_genre[music_genre == "?"] <- NA

# remove NAs
music_genre <- music_genre[complete.cases(music_genre),]

music_genre$tempo = as.numeric(music_genre$tempo)

normalize = function(x){
 (x - min(x)) / (max(x) - min(x))
}

# normalize popularity, loudness and valence columns
music_genre[c(1,7,10)] = lapply(music_genre[c(1,7,10)], normalize)

# collapse factors of key column to group sharps and naturals
music_genre$key = fct_collapse(music_genre$key, 
                               A = c("A", "A#"),
                               B = c("B", "B#"),
                               C = c("C", "C#"),
                               D = c("D", "D#"),
                               E = c("E", "E#"),
                               F = c("F", "F#"),
                               G = c("G", "G#"))

# change "Hip-Hop" value in genre column to a usable R name
music_genre[music_genre == "Hip-Hop"] <- "HipHop"

# rename genre column from "music_genre" to "genre"
names(music_genre)[names(music_genre)=="music_genre"] = "genre" 

# change mode and music_genre columns to factor
music_genre$mode = as.factor(music_genre$mode) 
music_genre$genre = as.factor(music_genre$genre)

lapply(music_genre[c(5,8,12)], table)
```

Each of the factor variables are well balanced within the dataset.

Boxplot of Danceability:

```{r}
# find the 75th percentile of danceable songs
boxplot(music_genre$danceability)

danceabilitySummary <- summary(music_genre$danceability)
danceabilitySummary

print("The 75th percentile of danceability includes values of .687 and above")

music_genre$danceability = (ifelse(music_genre$danceability > danceabilitySummary[5], 1, 0))
music_genre$danceability = fct_collapse(as.factor(music_genre$danceability), "bottom75" = "0", "top25" = "1")

# split up data into train, tune and test
set.seed(3001)
part1_indexing = createDataPartition(music_genre$danceability,
                                     times = 1,
                                     p = 0.70,
                                     groups=1,
                                     list=FALSE)

train = music_genre[part1_indexing,]
tune_and_test = music_genre[-part1_indexing,]

tune_and_test_index = createDataPartition(tune_and_test$danceability,
                                          p = .5,
                                          list = FALSE,
                                          times = 1)

tune = tune_and_test[tune_and_test_index,]
test = tune_and_test[-tune_and_test_index,]

# show nicely formatted table of cleaned data
datatable(music_genre) 
```

# Our Models {.tabset}

## KNN {.tabset}

### 25 Neighbors

For our KNN model, we started using 25 nearest neighbors and every variable in the dataset. This model performed well with the negative class, which, fortunately is the class we are more interested in correctly predicting. However, at a sensitivity of only 50%, we would miss one of every two songs that would be “danceable”, and that may be too low for the bars. 


```{r knn25}
KNN_train = train[-c(3,5,12)]
train1h = one_hot(as.data.table(KNN_train),cols = "auto",sparsifyNAs = TRUE,naCols = TRUE,dropCols = TRUE,dropUnusedLevels = TRUE)
KNN_tune = tune[-c(3,5,12)]
tune1h = one_hot(as.data.table(KNN_tune),cols = "auto",sparsifyNAs = TRUE,naCols = TRUE,dropCols = TRUE,dropUnusedLevels = TRUE)
Music_25NN = knn(train = train1h,
                test = tune1h,
                cl = train$danceability,
                k = 25,
                use.all = TRUE,
                prob = TRUE)

confusionMatrix(as.factor(Music_25NN), as.factor(tune$danceability), positive = "top25", dnn=c("Prediction", "Actual"), mode = "sens_spec")
#Establishing dataframe with both the prediction and probability
Music_25NN_Prob = data.frame(pred = as_factor(Music_25NN), prob = attr(Music_25NN, "prob"))
#Adjusting so probability aligns with probability of "top25" for all observations
Music_25NN_Prob$prob = ifelse(Music_25NN_Prob$pred == "bottom75", 1 - Music_25NN_Prob$prob, Music_25NN_Prob$prob)
#Finding F1 score at .5 threshold
pred_5_25NN = as_factor(ifelse(Music_25NN_Prob$prob > 0.5, "top25", "bottom75"))
print(paste("F-1 Score at a .5 threshold:",F1_Score(y_pred = pred_5_25NN, y_true = as_factor(tune$danceability), positive = "top25")))
```

We assumed popularity, energy, liveliness, loudness, and tempo would best align with danceability, so we attempted to feature engineer the KNN model by only using these variables. However, this led to major decreases in both sensitivity and accuracy.

```{r knnSelectVariables}
#Selecting the variables I predict would most closely correlate to danceability (popularity, energy, liveliness, loudness, and tempo)
Music_25NN_tuned = knn(train = train[c(1,4,6,7,10)],
                test = tune[c(1,4,6,7,10)],
                cl = train$danceability,
                k = 25,
                use.all = TRUE,
                prob = TRUE)
confusionMatrix(as.factor(Music_25NN_tuned), as.factor(tune$danceability), positive = "top25", dnn=c("Prediction", "Actual"), mode = "sens_spec")
#Worsened accuracy (especially sensitivity)
```

### 100 Neighbors

When adjusting to 100 neighbors, the specificity marginally increased; however, this likely occurred because higher k’s favor the more prevalent class. 

```{r knn100}

#Sensitivity drops a lot for a small improvement in specificity (likely more-so due to the imbalanced nature of the data than it predicting the observations better)
Music_100NN = knn(train = train1h,
                test = tune1h,
                cl = train$danceability,
                k = 100,
                use.all = TRUE,
                prob = TRUE)
confusionMatrix(as.factor(Music_100NN), as.factor(tune$danceability), positive = "top25", dnn=c("Prediction", "Actual"), mode = "sens_spec")

#Establishing dataframe with both the prediction and probability
Music_100NN_Prob = data.frame(pred = as_factor(Music_100NN), prob = attr(Music_100NN, "prob"))
#Adjusting so probability aligns with probability of "top25" for all observations
Music_100NN_Prob$prob = ifelse(Music_100NN_Prob$pred == "bottom75", 1 - Music_100NN_Prob$prob, Music_100NN_Prob$prob)
#Finding F1 score at .5 threshold
pred_5_100NN = as_factor(ifelse(Music_100NN_Prob$prob > 0.5, "top25", "bottom75"))
print(paste("F-1 Score at a .5 threshold:",F1_Score(y_pred = pred_5_100NN, y_true = as_factor(tune$danceability), positive = "top25")))

Music_100NN_tuned = knn(train = train[c(1,4,6,7,10)],
                test = tune[c(1,4,6,7,10)],
                cl = train$danceability,
                k = 25,
                use.all = TRUE,
                prob = TRUE)
confusionMatrix(as.factor(Music_100NN_tuned), as.factor(tune$danceability), positive = "top25", dnn=c("Prediction", "Actual"), mode = "sens_spec")
```

### 5 Neighbors

At only 5 neighbors, the specificity and accuracy decreased because the model did not have enough information to predict danceability well. Overall, our first model of 25 neighbors with all variables likely best fit our business question. 

```{r knn5}

Music_5NN = knn(train = train1h,
                test = tune1h,
                cl = train$danceability,
                k = 5,
                use.all = TRUE,
                prob = TRUE)
confusionMatrix(as.factor(Music_5NN), as.factor(tune$danceability), positive = "top25", dnn=c("Prediction", "Actual"), mode = "sens_spec")
#Slightly increased sensitivity, but not enough and at the expense of accuracy

#Establishing dataframe with both the prediction and probability
Music_5NN_Prob = data.frame(pred = as_factor(Music_5NN), prob = attr(Music_5NN, "prob"))
#Adjusting so probability aligns with probability of "top25" for all observations
Music_5NN_Prob$prob = ifelse(Music_5NN_Prob$pred == "bottom75", 1 - Music_5NN_Prob$prob, Music_5NN_Prob$prob)
#Finding F1 score at .5 threshold
pred_5_5NN = as_factor(ifelse(Music_5NN_Prob$prob > 0.5, "top25", "bottom75"))
print(paste("F-1 Score at a .5 threshold:",F1_Score(y_pred = pred_5_5NN, y_true = as_factor(tune$danceability), positive = "top25")))

Music_5NN_tuned = knn(train = train[c(1,4,6,7,10)],
                test = tune[c(1,4,6,7,10)],
                cl = train$danceability,
                k = 5,
                use.all = TRUE,
                prob = TRUE)
confusionMatrix(as.factor(Music_5NN_tuned), as.factor(tune$danceability), positive = "top25", dnn=c("Prediction", "Actual"), mode = "sens_spec")
#Best model is likely the first model made (the increased in specificity were not worth the decreases in sensitivity) 
```

## Decision Tree 

For our decision tree, we first cross-validated a c5.0 model to find the ideal number of boosting iterations along with whether or not it should winnow (remove) variables of low importance. Ultimately, we used 20 boosting iterations and no winnowing, but this process was very computationally expensive. 


```{r decisionTree1, echo=FALSE}

# This chunk is only used to learn the ideal winnowing/boosting methods, but it is computationally expensive, so it is not necessary to run after the first time
features = data.frame(train[-3])
target = train$danceability
  
#Creating a cross validation process that maximizes the F-1 Score
cv_train = trainControl(method = "repeatedcv",
                        number = 10,
                        repeats = 5,
                        returnResamp = "all",
                        classProbs = TRUE,
                        allowParallel = TRUE,)
grid_control = expand.grid(.winnow = c(TRUE,FALSE), 
                    .trials=c(1,5,10,15,20), 
                    .model="tree")
#Use this to get # of boosting iterations/winnowing through cross validation that prioritizes F-1 score (This is very computationally expensive)
dance_mdl = train(x=features,
                y=target,
                method="C5.0",
                tuneGrid=grid_control,
                trControl=cv_train)
plot(dance_mdl)
#Results: 20 Boosting iterations with no winnowing
#Remaking the model for visualization purposes
```

Our initial model performed better than the KNN model, but we can still improve through tuning. We adjusted the hyperparameters of:
1. The number of minimum cases in each leaf node
2. The confidence factor (the threshold of error allowed in the data; the higher the number, the less pruning in the model.


```{r decisionTree2, fig.height=5, fig.width=10}

c5_model = C5.0(danceability~.,
                    method = "class",
                    parms = list(split = "gini"),
                    data = train,
                    trials = 20,
                    control = C5.0Control(winnow = FALSE,
                                          minCases = 500))

varImp(c5_model)
plot(c5_model)

```

The first split is for genre; intuitively, this makes sense because of the booleans that follow. Rap and Hip hop will be most danceable when they are high energy/fast tempo while other genres may be more suited towards slow dancing that work best with low energy/slow tempo

```{r decisionTree3}
dance_prob = as_tibble(predict(c5_model, tune, type = "prob"))
dance_pred = predict(c5_model, tune, type = "class")
confusionMatrix(as.factor(dance_pred),
                as.factor(tune$danceability),
                dnn = c("Predicted", "Actual"),
                mode = "sens_spec",
                positive = "top25")
#table = table(as.factor(dance_pred),
#      as.factor(tune$danceability))
#(spec = table[1]/(table[1]+table[2]))
#F1 Score at .5 threshold
pred_5 = as_factor(ifelse(dance_prob$top25 > 0.5, "top25", "bottom75"))
print(paste("F-1 Score at a .5 threshold:",F1_Score(y_pred = pred_5, y_true = as_factor(tune$danceability), positive = "top25")))
```

### Tuning {.tabset} 

#### Number of Cases

```{r tuningCases1, include=FALSE}
empty = data.frame(min_cases = character(),
                   F1 = numeric())
min_cases = function(x){
  c5_adj_model = C5.0(danceability~.,
                    method = "class",
                    parms = list(split = "gini"),
                    data = train,
                    trials = 20,
                    control = C5.0Control(winnow = FALSE,
                                          minCases = x,
                                          ))
  c5_adj_pred = as_tibble(predict(c5_adj_model, tune, type = "prob"))
  c5_adj_class = as_factor(predict(c5_adj_model, tune))
  pred_4_adj = as.factor(ifelse(c5_adj_pred$top25 > 0.5, "top25", "bottom75"))
  pred_4_adj = relevel(pred_4_adj, "bottom75")
  table = table(as.factor(c5_adj_class),
      as.factor(tune$danceability))
  spec = table[1]/(table[1]+table[2])
  F_1 = data.frame(min_cases = x,F1 = F1_Score(y_pred = pred_4_adj, y_true = as_factor(tune$danceability), positive = "top25"), specificity = spec)
  
  empty <<- rbind(empty, F_1)
}
cases = c(2,5,10,50,100,250,500,1000,2500)
lapply(cases, min_cases)
```


```{r tuningCases2}
ggplot(empty, aes(x = reorder(as.factor(min_cases), -F1), y = F1))+ geom_col(width = .8)+ geom_bar(data=subset(empty, min_cases == 50), aes(as.factor(min_cases), F1),
              fill="green", stat="identity")
ggplot(empty, aes(x = reorder(as.factor(min_cases), -specificity), y = specificity))+ geom_col(width = .8)+ geom_bar(data=subset(empty, min_cases == 50), aes(as.factor(min_cases), specificity),
              fill="green", stat="identity")
#50 seems to be the best compromise
```

#### Confidence Factor

```{r tuningConfidenceFactor1, include=FALSE}
empty2 = data.frame(CF_level = character(),
                   F1 = numeric())
CF_level = function(x){
  c5_adj_model = C5.0(danceability~.,
                    method = "class",
                    parms = list(split = "gini"),
                    data = train,
                    trials = 20,
                    control = C5.0Control(winnow = FALSE,
                                          minCases = 50,
                                          CF = x
                                          ))
  c5_adj_pred = as_tibble(predict(c5_adj_model, tune, type = "prob"))
  c5_adj_class = as_factor(predict(c5_adj_model, tune))
  pred_4_adj = as.factor(ifelse(c5_adj_pred$top25 > 0.5, "top25", "bottom75"))
  pred_4_adj = relevel(pred_4_adj, "bottom75")
  table = table(as.factor(c5_adj_class),
      as.factor(tune$danceability))
  spec = table[1]/(table[1]+table[2])
  F_1 = data.frame(CF_level = x,F1 = F1_Score(y_pred = pred_4_adj, y_true = as_factor(tune$danceability), positive = "top25"), specificity = spec)
  
  empty2 <<- rbind(empty2, F_1)
}
cases = c(.1,.2,.3,.4,.5,.6,.7,.8,.9)
lapply(cases, CF_level)

```

``` {r tuningConfidenceFactor2}
ggplot(empty2, aes(x = reorder(as.factor(CF_level), -F1), y = F1))+ geom_col(width = .8)+ geom_bar(data=subset(empty2, CF_level==.9), aes(as.factor(CF_level), F1),
              fill="green", stat="identity")

ggplot(empty2, aes(x = reorder(as.factor(CF_level), -specificity), y = specificity))+ geom_col(width = .8)+ geom_bar(data=subset(empty2, CF_level==.9), aes(as.factor(CF_level), specificity),
              fill="green", stat="identity")
```

### Final Model Against Tune Data 

We found 50 cases and a confidence factor of .9 to be the best compromise between F1-score and specificity. The final model marginally decreased in specificity but greatly increased in sensitivity and F1.

```{r decisionTreeFinalModel}
c5_model_tune = C5.0(danceability~.,
                    method = "class",
                    parms = list(split = "gini"),
                    data = train,
                    trials = 20,
                    control = C5.0Control(winnow = FALSE,
                                          minCases = 50,
                                          CF = .9))
dance_prob_tune = as_tibble(predict(c5_model_tune, tune, type = "prob"))
dance_pred_tune = predict(c5_model_tune, tune, type = "class")
confusionMatrix(as.factor(dance_pred_tune),
                as.factor(tune$danceability),
                dnn = c("Predicted", "Actual"),
                mode = "sens_spec",
                positive = "top25")
#F1 Score at .5 threshold
pred_5_tune = as_factor(ifelse(dance_prob_tune$top25 > 0.5, "top25", "bottom75"))
print(paste("F-1 Score at a .5 threshold:",F1_Score(y_pred = pred_5_tune, y_true = as_factor(tune$danceability), positive = "top25")))
#Best model so far, should do roc curve to find threshold
dance_prob_tune = as_tibble(dance_prob_tune)
dance_eval_tune = tibble(pred_class=dance_pred_tune, pred_prob=dance_prob_tune$top25,target=as.numeric(tune$danceability))
pred = prediction(dance_eval_tune$pred_prob, dance_eval_tune$target)
#Choosing to evaluate the true positive rate and false positive rate based on the threshold
ROC_curve = performance(pred,"tpr","fpr")
plot(ROC_curve, colorize=TRUE)
abline(a=0, b= 1)
tree_perf_AUC = performance(pred,"auc")
print(paste("AUC =",tree_perf_AUC@y.values))
#.5 is near the elbow of the graph (maybe could try .4 and .6)
```

### Thresholding {.tabset}
#### @ .4

```{r thresholding4}
dance_pred_4 = as_factor(ifelse(dance_prob$top25 > 0.4, "top25", "bottom75"))
confusionMatrix(as.factor(dance_pred_4),
                as.factor(tune$danceability),
                dnn = c("Predicted", "Actual"),
                mode = "sens_spec",
                positive = "top25")
```

The increases in sensitivity are not worth the decreases in specificity because it is our metric of interest.

#### @ .6

```{r}

dance_pred_6 = as_factor(ifelse(dance_prob$top25 > 0.6, "top25", "bottom75"))
confusionMatrix(as.factor(dance_pred_6),
                as.factor(tune$danceability),
                dnn = c("Predicted", "Actual"),
                mode = "sens_spec",
                positive = "top25")
```

The improvements in specificity are not worth the great losses in sensitivity.

## Random Forest {.tabset}

For our random forest model, we started with an initial model with 500 trees and 3 randomly sampled variables. We then created 7 more models with 250, 500, 750 and 1000 trees and 3 and 4 randomly sampled variables excluding the model with 500 trees and 3 randomly sampled variables as this is the same as the initial model. For each model, we looked at the variable importance as well as a graph of error rates against the number of trees. We also looked at several evaluation metrics including specificity and F-1 score. We determined the best of the 8 model variations we tried to be the one with 1000 trees and 4 randomly sampled variables. This had the highest specificity and F-1 score of the random forest models we created and also does well in predicting the negative class, similarly to the decision tree model.

```{r randomForest, results=FALSE}
# function to calculate the mtry level 
mytry_tune <- function(x){
  xx <- dim(x)[2]-1
  sqrt(xx)
}
mytry_tune(music_genre)

set.seed(3001)
rfInit = randomForest(danceability~.,      #<- Formula: response variable ~ predictors.
                      #   The period means 'use all other variables in the data'.
                      train,               #<- A data frame with the variables to be used.
                      #y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
                      #subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
                      #xtest = NULL,       #<- This is already defined in the formula by the ".".
                      #ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
                      ntree = 500,         #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
                      mtry = 3,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
                      replace = TRUE,      #<- Should sampled data points be replaced.
                      #classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population 
                      #strata = NULL,      #<- Not necessary for our purpose here.
                      sampsize = 3000,     #<- Size of sample to draw each time.
                      nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
                      #maxnodes = NULL,    #<- Limits the number of maximum splits. 
                      importance = TRUE,   #<- Should importance of predictors be assessed?
                      #localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
                      proximity = FALSE,   #<- Should a proximity measure between rows be calculated?
                      norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
                      do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
                      keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
                      keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees?

# function to show call, variable importance and confusion matrix given a model
showModelOutput <- function(mdl, modelName) {
  print("Call")
  print(mdl$call)
  print('Variable Importance')
  print(mdl$importance)
  varImpPlot(mdl, main=modelName)
  plot(mdl, main=modelName)
  mdlPredict = predict(mdl,
                       tune,
                       type = "response",
                       predict.all = FALSE,
                       proximity = FALSE)
  confusionMatrix(as.factor(mdlPredict),
                as.factor(tune$danceability),
                dnn = c("Predicted", "Actual"),
                mode = "sens_spec",
                positive = "top25")
}

# function to output an F-1 score given a model
f1score = function(mdl){  
  mdlPredictprob = as_tibble(predict(mdl,
                    tune,
                    type = "prob",
                    predict.all = FALSE,
                    proximity = FALSE))
  
  
  pred_5_mdl = as_factor(ifelse(mdlPredictprob$top25 > 0.5, "top25", "bottom75"))
  print(paste("F-1 Score at a .5 threshold:",F1_Score(y_pred = pred_5_mdl, y_true = as_factor(tune$danceability), positive = "top25")))
}

# function to run the model with a given number of trees and randomly sampled variables
tuneModel <- function(numTrees, mTry) {
  set.seed(3001)	
  rf = randomForest(danceability~.,      #<- Formula: response variable ~ predictors.
                    #   The period means 'use all other variables in the data'.
                    train,               #<- A data frame with the variables to be used.
                    #y = NULL,           #<- A response vector. This is unnecessary because we're specifying a response formula.
                    #subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
                    #xtest = NULL,       #<- This is already defined in the formula by the ".".
                    #ytest = NULL,       #<- This is already defined in the formula by "PREGNANT".
                    ntree = numTrees,    #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
                    mtry = mTry,         #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
                    replace = TRUE,      #<- Should sampled data points be replaced.
                    #classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population 
                    #strata = NULL,      #<- Not necessary for our purpose here.
                    sampsize = 3000,     #<- Size of sample to draw each time.
                    nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
                    #maxnodes = NULL,    #<- Limits the number of maximum splits. 
                    importance = TRUE,   #<- Should importance of predictors be assessed?
                    #localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
                    proximity = FALSE,   #<- Should a proximity measure between rows be calculated?
                    norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
                    do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
                    keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
                    keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees?
  return(rf)
}

rf1 = tuneModel(250, 3)
rf2 = tuneModel(750, 3)
rf3 = tuneModel(1000, 3)
rf4 = tuneModel(250, 4)
rf5 = tuneModel(500, 4)
rf6 = tuneModel(750, 4)
rf7 = tuneModel(1000, 4)
```

### Initial Model
```{r rfInit}
showModelOutput(rfInit, 'Initial Model')
f1score(rfInit)
```

### 250 Trees with 3 Randomly Sampled Variables
```{r rf1}
showModelOutput(rf1, '250 Trees with 3 mtry')
f1score(rf1)
```

### 750 Trees with 3 Randomly Sampled Variables
```{r rf2}
showModelOutput(rf2, '750 Trees with 3 mtry')
f1score(rf2)
```

### 1000 Trees with 3 Randomly Sampled Variables
```{r rf3}
showModelOutput(rf3, '1000 Trees with 3 mtry')
f1score(rf3)
```

### 250 Trees with 4 Randomly Sampled Variables
```{r rf4}
showModelOutput(rf4, '250 Trees with 4 mtry')
f1score(rf4)
```

### 500 Trees with 4 Randomly Sampled Variables
```{r rf5}
showModelOutput(rf5, '500 Trees with 4 mtry')
f1score(rf5)
```

### 750 Trees with 4 Randomly Sampled Variables
```{r rf6}
showModelOutput(rf6, '750 Trees with 4 mtry')
f1score(rf6)
```

### 1000 Trees with 4 Randomly Sampled Variables
```{r rf7}
showModelOutput(rf7, '1000 Trees with 4 mtry')
f1score(rf7)
```

# Testing our Final Models {.tabset}

## KNN

```{r knnTest}
KNN_test = test[-c(3,5,12)]
test1h = one_hot(as.data.table(KNN_test),cols = "auto",sparsifyNAs = TRUE,naCols = TRUE,dropCols = TRUE,dropUnusedLevels = TRUE) 
Music_25NN_Final = knn(train = train1h,
                test = test1h,
                cl = train$danceability,
                k = 25,
                use.all = TRUE,
                prob = TRUE)
confusionMatrix(as.factor(Music_25NN_Final), as.factor(test$danceability), positive = "top25", dnn=c("Prediction", "Actual"), mode = "sens_spec")
#Establishing dataframe with both the prediction and probability
Music_25NN_Final_Prob = data.frame(pred = as_factor(Music_25NN_Final), prob = attr(Music_25NN_Final, "prob"))
#Adjusting so probability aligns with probability of "top25" for all observations
Music_25NN_Final_Prob$prob = ifelse(Music_25NN_Final_Prob$pred == "bottom75", 1 - Music_25NN_Final_Prob$prob, Music_25NN_Final_Prob$prob)
#Finding F1 score at .5 threshold
pred_5_KNN = as_factor(ifelse(Music_25NN_Final_Prob$prob > 0.5, "top25", "bottom75"))
print(paste("F-1 Score at a .5 threshold:",F1_Score(y_pred = pred_5_KNN, y_true = as_factor(test$danceability), positive = "top25")))
```

## Decision Tree

```{r decisionTreeTest}
dance_prob_test = as_tibble(predict(c5_model_tune, test, type = "prob"))
dance_pred_test = predict(c5_model_tune, test, type = "class")
confusionMatrix(as.factor(dance_pred_test),
                as.factor(test$danceability),
                dnn = c("Predicted", "Actual"),
                mode = "sens_spec",
                positive = "top25")
#F1 Score at .5 threshold
pred_5_test = as_factor(ifelse(dance_prob_test$top25 > 0.5, "top25", "bottom75"))
print(paste("F-1 Score at a .5 threshold:",F1_Score(y_pred = pred_5_test, y_true = as_factor(test$danceability), positive = "top25")))
```

## Random Forest

```{r randomForestTest}
rfPredict = predict(rf7,
                    test,
                    type = "response",
                    predict.all = FALSE,
                    proximity = FALSE)
rfPredictprob = as_tibble(predict(rf7,
                    test,
                    type = "prob",
                    predict.all = FALSE,
                    proximity = FALSE))
confusionMatrix(as.factor(rfPredict),
                as.factor(test$danceability),
                dnn = c("Predicted", "Actual"),
                mode = "sens_spec",
                positive = "top25")
pred_5_rf = as_factor(ifelse(rfPredictprob$top25 > 0.5, "top25", "bottom75"))
print(paste("F-1 Score at a .5 threshold:",F1_Score(y_pred = pred_5_rf, y_true = as_factor(test$danceability), positive = "top25")))
```

# Overall Findings

Moving forward, we would endorse this model, especially if the bars are particularly interested in filtering out "non-danceable" songs. Bars should firstly keep in mind the genre of songs that resonates with their patrons and pass that genre of songs through the machine learning algorithm. The models predict differently based on the genre because a "danceble" classical song greatly differs from a "danceable" Hip Hop song.

We recommend the bars use our single tree model if they value correctly predicting "dancy" songs and the random forest if they want to ensure they do not play a buzzkill because they are more conservative. Additionally, if the preferences of customers change often and the danceability score shifts as time goes on, the bars may want to use the KNN model because it re-trains each time it is run and is not computationally expensive. 

Because this dataset observes songs, we do not have any protected classes and a fairness assessment is necessarily not warranted.

# Next Steps

To improve our model, we could reduce the sample space of the negative class to make the dataset more balanced. This would leave us with a more clear metric of interest (specificity) and the model might improve its ability to predict both classes in tandem. 

We believe other variables of interest could include **days since released** (the more recent the song, the more likely it will currently be considered "danceable") and **Billboard top 100** as a binary variable that indicates whether or not the song ever reached the top 100 of the charts. 

In the future, it may be productive to try the model using regression methods instead of classification because the danceability variable begins as a continuous variable.

With all of that being said, the biggest limitation of this model exists in the data itself. We do not know how "danceability" is determined, and the interpretation of that variable is subjective rather than objective. Therefore, the bars should continue to evaluate whether or not the number of their guests do increase and more people dance at their bar. Secondly, if the model does succeed, the bars should monitor whether sales on drinks also increase. Despite the increase in traffic, the number of drinks purchased could decrease if guests choose to continue dancing instead of taking a break to buy a drink. In that case, the business question should be adapted to answer a question more correlated with alcohol sales.
